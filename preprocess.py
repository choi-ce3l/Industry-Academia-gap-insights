import pandas as pd
import re
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import nltk

# Download the 'punkt_tab' resource along with other resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')  # Download the missing resource


# ----------------------------------
# ğŸ”§ ê°œë³„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
# ----------------------------------
def preprocess_text(text):
    if pd.isnull(text):
        return ""

    # HTML/XML ì œê±°
    text = BeautifulSoup(text, "html.parser").get_text()

    # LaTeX ì œê±°
    text = re.sub(r"\$.*?\$", "", text)
    text = re.sub(r"\\\[.*?\\\]", "", text)
    text = re.sub(r"\\\(.*?\\\)", "", text)

    # íŠ¹ìˆ˜ê¸°í˜¸, ìˆ«ì ì œê±°
    text = re.sub(r"[^a-zA-Z\s]", " ", text)

    # ì†Œë¬¸ì ë³€í™˜
    text = text.lower()

    # í† í°í™”
    tokens = word_tokenize(text)

    # ë¶ˆìš©ì–´ ì œê±°
    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]

    # í‘œì œì–´ ì¶”ì¶œ
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(w) for w in tokens]

    return " ".join(tokens)


# ----------------------------------
# ğŸ“¦ ì „ì²´ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜
# ----------------------------------
def preprocess_and_vectorize(df, text_columns, method='count', max_features=5000):
    """
    ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì „ì²˜ë¦¬í•˜ê³ , í•˜ë‚˜ë¡œ í•©ì³ ë²¡í„°í™”ê¹Œì§€ ìˆ˜í–‰

    Parameters:
        df (pd.DataFrame): ì…ë ¥ ë°ì´í„°
        text_columns (list): ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ëª©ë¡
        method (str): 'count' or 'tfidf'
        max_features (int): ë²¡í„°í™” ì‹œ ìµœëŒ€ ë‹¨ì–´ ìˆ˜

    Returns:
        vectorizer, vectorized_matrix: í›ˆë ¨ëœ ë²¡í„°ë¼ì´ì €ì™€ ë²¡í„° í–‰ë ¬
    """

    # ê° í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì „ì²˜ë¦¬ â†’ í•©ì¹˜ê¸°
    print("ğŸ”„ ì „ì²˜ë¦¬ ì¤‘...")
    for col in text_columns:
        df[f'{col}_clean'] = df[col].apply(preprocess_text)

    df['combined_text'] = df[[f'{col}_clean' for col in text_columns]].agg(' '.join, axis=1)

    # ë²¡í„°ë¼ì´ì € ì„ íƒ
    if method == 'count':
        vectorizer = CountVectorizer(max_features=max_features, stop_words='english')
    elif method == 'tfidf':
        vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')
    else:
        raise ValueError("methodëŠ” 'count' ë˜ëŠ” 'tfidf' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    print(f"âœ… '{method}' ë°©ì‹ìœ¼ë¡œ ë²¡í„°í™” ì¤‘...")
    vectorized_matrix = vectorizer.fit_transform(df['combined_text'])

    print("ğŸ‰ ë²¡í„°í™” ì™„ë£Œ! í–‰ë ¬ í¬ê¸°:", vectorized_matrix.shape)
    return vectorizer, vectorized_matrix