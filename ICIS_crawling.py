import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin
import pandas as pd

# í•™íšŒ í˜ì´ì§€ì—ì„œ ì¹´í…Œê³ ë¦¬ URL ì¶”ì¶œ
def get_category_urls(conference_url):
    response = requests.get(conference_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    a_tags = soup.select('#events-listing dt a')
    return [a.get('href') for a in a_tags]

# ê° ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ URL ì¶”ì¶œ
def get_paper_urls(category_url):
    response = requests.get(category_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    a_tags = soup.select('#series-home > table > tbody > tr > td > p > a')
    return [a.get('href') for a in a_tags]

# ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ (PDF ë§í¬ í¬í•¨)
def get_paper_info(paper_url):
    response = requests.get(paper_url)
    soup = BeautifulSoup(response.text, 'html.parser')

    title = soup.select_one('#title > h1 > a')
    title = title.text.strip() if title else None

    authors = [a.text.strip() for a in soup.select('#authors > p')]
    authors_str = ', '.join(authors)

    paper_number = soup.select_one('#paper_number p')
    paper_number = paper_number.text.strip() if paper_number else None

    abstract = soup.select_one('#abstract > p')
    abstract = abstract.text.strip() if abstract else None

    pdf_tag = soup.select_one('#pdf')
    pdf_url = urljoin(paper_url, pdf_tag.get('href')) if pdf_tag else None

    return {
        'title': title,
        'authors': authors_str,
        'paper_number': paper_number,
        'abstract': abstract,
        'pdf_url': pdf_url,
        'paper_url': paper_url
    }

# PDF ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
def download_pdf_from_url(pdf_url, filename):
    response = requests.get(pdf_url)
    if response.status_code == 200:
        with open(filename, "wb") as f:
            f.write(response.content)
        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename}")
    else:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({filename}) - ìƒíƒœì½”ë“œ: {response.status_code}")

# ì „ì²´ ì‹¤í–‰
if __name__ == "__main__":
    base_url = 'https://aisel.aisnet.org'
    conference_url = urljoin(base_url, 'icis2021/')
    save_folder = 'icis2021_papers'
    os.makedirs(save_folder, exist_ok=True)

    # ëª¨ë“  ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ì €ì¥ìš©
    all_papers = []

    # ì¹´í…Œê³ ë¦¬ ìˆœíšŒ
    category_urls = get_category_urls(conference_url)
    for cat_idx, category_url in enumerate(category_urls):
        print(f"\nğŸ“ ì¹´í…Œê³ ë¦¬ {cat_idx + 1}/{len(category_urls)}: {category_url}")
        paper_urls = get_paper_urls(category_url)

        for paper_idx, paper_url in enumerate(paper_urls):
            full_paper_url = urljoin(base_url, paper_url)
            paper_info = get_paper_info(full_paper_url)

            filename = f"{cat_idx + 1:02}_{paper_idx + 1:03}.pdf"
            filepath = os.path.join(save_folder, filename)

            if paper_info['pdf_url']:
                download_pdf_from_url(paper_info['pdf_url'], filepath)
                paper_info['pdf_filename'] = filename
            else:
                paper_info['pdf_filename'] = None
                print(f"âŒ PDF ë§í¬ ì—†ìŒ: {full_paper_url}")

            all_papers.append(paper_info)

    # DataFrame ìƒì„± ë° ì €ì¥
    df = pd.DataFrame(all_papers)
    df.to_csv("icis2021_metadata.csv", index=False)
    print("âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: icis2021_metadata.csv")