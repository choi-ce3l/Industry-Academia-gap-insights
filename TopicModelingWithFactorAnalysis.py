import pandas as pd
import numpy as np
import re
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import FactorAnalysis
from gensim import corpora, models
from gensim.models.coherencemodel import CoherenceModel
import matplotlib.pyplot as plt
import os

# ----------------------------------
# ğŸ”§ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
# ----------------------------------
def preprocess_text(text):
    if pd.isnull(text):
        return ""

    if isinstance(text, list):
        text = ' '.join(text)

    # HTML/XML ì œê±°
    if isinstance(text, str) and ('<' in text and '>' in text):
        text = BeautifulSoup(text, "html.parser").get_text()
    text = re.sub(r"\$.*?\$", "", text)
    text = re.sub(r"\\\(.*?\\\)", "", text)
    text = re.sub(r"[^a-zA-Z\s]", " ", text)
    text = text.lower()

    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]

    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(w) for w in tokens]

    return " ".join(tokens)

# ----------------------------------
# ğŸ“¦ ì „ì²˜ë¦¬ ë° ë²¡í„°í™”
# ----------------------------------
def preprocess_and_vectorize(df, method='count', max_features=5000, data_type='journal'):
    if data_type == 'journal':
        text_columns = ['title', 'abstract', 'keywords']
    elif data_type == 'article':
        text_columns = ['title', 'content', 'keywords']
    else:
        raise ValueError("data_typeì€ 'journal' ë˜ëŠ” 'article' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    for col in text_columns:
        if col not in df.columns:
            df[col] = ''
        df[f'{col}_clean'] = df[col].apply(preprocess_text)

    df['combined_text'] = df[[f'{col}_clean' for col in text_columns]].agg(' '.join, axis=1)

    if method == 'count':
        vectorizer = CountVectorizer(max_features=max_features, stop_words='english', max_df=0.90)
    elif method == 'tfidf':
        vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english', max_df=0.90)
    else:
        raise ValueError("methodëŠ” 'count' ë˜ëŠ” 'tfidf' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    vectorized_matrix = vectorizer.fit_transform(df['combined_text'])
    return vectorizer, vectorized_matrix

# ----------------------------------
# ğŸ” ë¬¸ì„œ ì¤€ë¹„ ë° í† í°í™”
# ----------------------------------
def prepare_documents(df, data_type='journal'):
    df = df.fillna('')

    if 'combined_text' not in df.columns:
        if data_type == 'journal':
            text_columns = ['title', 'abstract', 'keywords']
        elif data_type == 'article':
            text_columns = ['title', 'content', 'keywords']
        else:
            raise ValueError("data_typeì€ 'journal' ë˜ëŠ” 'article' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        for col in text_columns:
            if col not in df.columns:
                df[col] = ''
        df['combined_text'] = df[text_columns].agg(' '.join, axis=1)
        df['combined_text'] = df['combined_text'].apply(preprocess_text)

    return [doc.split() for doc in df['combined_text']]

# ----------------------------------
# ğŸ“ˆ Coherence ì ìˆ˜ ê³„ì‚°
# (ì—¬ê¸°ì„œëŠ” ì•ˆì“°ì§€ë§Œ ë‚¨ê²¨ë‘ )
# ----------------------------------
def compute_coherence_scores(dictionary, corpus, texts, start, limit, step):
    scores = []
    for k in range(start, limit, step):
        lda = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, random_state=42, passes=10)
        cm = CoherenceModel(model=lda, texts=texts, dictionary=dictionary, coherence='c_v')
        scores.append((k, cm.get_coherence()))
    return scores

# ----------------------------------
# ğŸ§© í† í”½-ë¬¸ì„œ í–‰ë ¬ ìƒì„±
# ----------------------------------
def extract_topic_matrix(lda_model, corpus, num_topics):
    topic_matrix = []
    for doc in corpus:
        dist = lda_model.get_document_topics(doc, minimum_probability=0)
        topic_matrix.append([prob for _, prob in sorted(dist)])
    return pd.DataFrame(topic_matrix, columns=[f"Topic_{i}" for i in range(num_topics)])

# ----------------------------------
# ğŸ“Š ìš”ì¸ ë¶„ì„
# ----------------------------------
def run_factor_analysis(topic_df, n_factors=5, max_iter=500):
    fa = FactorAnalysis(n_components=n_factors, random_state=42, max_iter=max_iter)
    factors = fa.fit_transform(topic_df)
    loadings = pd.DataFrame(fa.components_.T, index=topic_df.columns, columns=[f"Factor_{i+1}" for i in range(n_factors)])
    return pd.DataFrame(factors, columns=loadings.columns), loadings

# ----------------------------------
# ğŸ“Š ìš”ì¸ ë¶„ì„ ìƒìœ„ ë¬¸ì„œ 5ê°œ ì €ì¥
# ----------------------------------
# ----------------------------------
# ğŸ“Š ìš”ì¸ ë¶„ì„ ìƒìœ„ ë¬¸ì„œ 5ê°œ ì €ì¥ (500ê¸€ì ì œí•œ ì¶”ê°€)
# ----------------------------------
def top_docs_by_factor(factor_df, docs_df, top_n=5, output_path='top_documents_by_factor.txt', data_type='journal'):
    with open(output_path, 'w', encoding='utf-8') as f:
        for factor in factor_df.columns:
            f.write(f"\nğŸ“Œ ìƒìœ„ ë¬¸ì„œ - {factor}\n")
            f.write("="*60 + "\n")
            top_indices = factor_df[factor].nlargest(top_n).index

            for i in top_indices:
                row = docs_df.loc[i]

                f.write(f"ğŸ“† Date: {row.get('date', '')}\n")
                f.write(f"ğŸ“„ Title: {row.get('title', '')}\n")

                if data_type == 'journal':
                    content = row.get('abstract', '')
                elif data_type == 'article':
                    content = row.get('content', '')
                else:
                    raise ValueError("data_typeì€ 'journal' ë˜ëŠ” 'article' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

                # ğŸ”µ ë³¸ë¬¸ 500ê¸€ìê¹Œì§€ë§Œ ì €ì¥
                if len(content) > 500:
                    content = content[:500].rstrip() + "..."

                f.write(f"ğŸ” Content (500ì ì´ë‚´): {content}\n")
                f.write(f"ğŸ·ï¸ Keywords: {row.get('keywords', '')}\n")
                f.write("-"*60 + "\n")

    print(f"âœ… ì €ì¥ ì™„ë£Œ (ë³¸ë¬¸ 500ì ì œí•œ ì ìš©): {output_path}")

# ----------------------------------
# ğŸ“ LDA ëª¨ë¸ë¡œë¶€í„° í† í”½ë³„ í‚¤ì›Œë“œ ì €ì¥
# ----------------------------------
def save_lda_topics(lda_model, num_words, output_path):
    with open(output_path, 'w', encoding='utf-8') as f:
        for idx, topic in lda_model.show_topics(num_topics=-1, num_words=num_words, formatted=False):
            keywords = ", ".join([word for word, _ in topic])
            f.write(f"Topic {idx}: {keywords}\n")
    print(f"âœ… LDA í† í”½ ì €ì¥ ì™„ë£Œ: {output_path}")

# ----------------------------------
# ğŸ¯ ë…„ë„ë³„ë¡œ LDA + Factor ë¶„ì„
# ----------------------------------
def run_yearly_lda_factor_analysis(df, data_type='journal', n_topics=10, n_factors=5, vectorizer_method='tfidf', max_features=5000, output_dir='results'):
    os.makedirs(output_dir, exist_ok=True)

    years = sorted(df['date'].unique())

    for year in years:
        print(f"ğŸ”µ Processing year: {year}")

        # í•´ë‹¹ ì—°ë„ ë°ì´í„° ì¶”ì¶œ
        year_df = df[df['date'] == year].reset_index(drop=True)

        # ë²¡í„°í™”
        vectorizer, vectorized_matrix = preprocess_and_vectorize(year_df, method=vectorizer_method, max_features=max_features, data_type=data_type)

        # Gensim LDAìš© corpus ì¤€ë¹„
        processed_docs = prepare_documents(year_df, data_type=data_type)
        dictionary = corpora.Dictionary(processed_docs)
        corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

        # LDA ëª¨ë¸ í•™ìŠµ
        lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topics, random_state=42, passes=10)

        # ğŸ¯ LDA í† í”½ë³„ í‚¤ì›Œë“œ ì €ì¥ ì¶”ê°€
        save_lda_topics(
            lda_model=lda_model,
            num_words=10,  # í† í”½ë‹¹ ìƒìœ„ 10ê°œ ë‹¨ì–´
            output_path=f"{output_dir}/02_{data_type}_{year}_lda_topics.txt"
        )

        # í† í”½-ë¬¸ì„œ í–‰ë ¬ ìƒì„±
        topic_df = extract_topic_matrix(lda_model, corpus, n_topics)

        # Factor Analysis
        factor_df, loadings = run_factor_analysis(topic_df, n_factors=n_factors)

        # ê²°ê³¼ ì €ì¥
        topic_df.to_csv(f"{output_dir}/02_{data_type}_{year}_topic_matrix.csv", index=False)
        factor_df.to_csv(f"{output_dir}/02_{data_type}{year}_factor_scores.csv", index=False)
        loadings.to_csv(f"{output_dir}/02_{data_type}{year}_factor_loadings.csv", index=True)

        top_docs_by_factor(
            factor_df=factor_df,
            docs_df=year_df,
            top_n=5,
            output_path=f"{output_dir}/02_{data_type}_{year}_top_docs_by_factor.txt",  # ì—¬ê¸° ìˆ˜ì •
            data_type=data_type
        )

        print(f"âœ… Year {year} ì™„ë£Œ! (Topic Matrix, Factor Scores, Loadings, Top Docs ì €ì¥)")


''' ì‚¬ìš© ë°©ë²•
run_yearly_lda_factor_analysis(
    df,  # ğŸ‘‰ ë‹¹ì‹ ì´ ë§Œë“  ë°ì´í„°í”„ë ˆì„
    data_type='journal',  # ğŸ‘‰ ë…¼ë¬¸ì´ë©´ 'journal', ë‰´ìŠ¤ ê¸°ì‚¬ë©´ 'article'
    n_topics=10,          # ğŸ‘‰ ê³ ì •: LDA í† í”½ ê°œìˆ˜
    n_factors=5,          # ğŸ‘‰ ê³ ì •: Factor Analysis ìš”ì¸ ìˆ˜
    vectorizer_method='tfidf',  # ğŸ‘‰ 'count'ë‚˜ 'tfidf' ì¤‘ ì„ íƒ ('tfidf' ì¶”ì²œ)
    max_features=5000,     # ğŸ‘‰ ë²¡í„°ë¼ì´ì € ìµœëŒ€ ë‹¨ì–´ ìˆ˜ (ì„ íƒì‚¬í•­)
    output_dir='data/result/02'   # ğŸ‘‰ ê²°ê³¼ ì €ì¥ í´ë”ëª…
)
'''