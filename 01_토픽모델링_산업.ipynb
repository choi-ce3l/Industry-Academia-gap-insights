{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T01:00:01.617691Z",
     "start_time": "2025-04-29T00:59:59.903795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ----------------------------------\n",
    "# ğŸ”§ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# ----------------------------------\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # HTML/XML ì œê±°\n",
    "    if isinstance(text, str) and ('<' in text and '>' in text):\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"\\$.*?\\$\", \"\", text)\n",
    "    text = re.sub(r\"\\\\\\(.*?\\\\\\)\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ----------------------------------\n",
    "# ğŸ“¦ ì „ì²˜ë¦¬ ë° ë²¡í„°í™”\n",
    "# ----------------------------------\n",
    "def preprocess_and_vectorize(df, method='count', max_features=5000, data_type='journal'):\n",
    "    if data_type == 'journal':\n",
    "        text_columns = ['title', 'abstract', 'keywords']\n",
    "    elif data_type == 'article':\n",
    "        text_columns = ['title', 'content', 'keywords']\n",
    "    else:\n",
    "        raise ValueError(\"data_typeì€ 'journal' ë˜ëŠ” 'article' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    for col in text_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''\n",
    "        df[f'{col}_clean'] = df[col].apply(preprocess_text)\n",
    "\n",
    "    df['combined_text'] = df[[f'{col}_clean' for col in text_columns]].agg(' '.join, axis=1)\n",
    "\n",
    "    if method == 'count':\n",
    "        vectorizer = CountVectorizer(max_features=max_features, stop_words='english', max_df=0.90)\n",
    "    elif method == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english', max_df=0.90)\n",
    "    else:\n",
    "        raise ValueError(\"methodëŠ” 'count' ë˜ëŠ” 'tfidf' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    vectorized_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "    return vectorizer, vectorized_matrix\n",
    "\n",
    "# ----------------------------------\n",
    "# ğŸ” ë¬¸ì„œ ì¤€ë¹„ ë° í† í°í™”\n",
    "# ----------------------------------\n",
    "def prepare_documents(df, data_type='journal'):\n",
    "    df = df.fillna('')\n",
    "\n",
    "    if 'combined_text' not in df.columns:\n",
    "        if data_type == 'journal':\n",
    "            text_columns = ['title', 'abstract', 'keywords']\n",
    "        elif data_type == 'article':\n",
    "            text_columns = ['title', 'content', 'keywords']\n",
    "        else:\n",
    "            raise ValueError(\"data_typeì€ 'journal' ë˜ëŠ” 'article' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "        for col in text_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = ''\n",
    "        df['combined_text'] = df[text_columns].agg(' '.join, axis=1)\n",
    "        df['combined_text'] = df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "    return [doc.split() for doc in df['combined_text']]\n",
    "\n",
    "# ----------------------------------\n",
    "# ğŸ“ˆ Coherence ì ìˆ˜ ê³„ì‚°\n",
    "# (ì—¬ê¸°ì„œëŠ” ì•ˆì“°ì§€ë§Œ ë‚¨ê²¨ë‘ )\n",
    "# ----------------------------------\n",
    "def compute_coherence_scores(dictionary, corpus, texts, start, limit, step):\n",
    "    scores = []\n",
    "    for k in range(start, limit, step):\n",
    "        lda = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, random_state=42, passes=10)\n",
    "        cm = CoherenceModel(model=lda, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        scores.append((k, cm.get_coherence()))\n",
    "    return scores\n",
    "\n",
    "# ----------------------------------\n",
    "# ğŸ§© í† í”½-ë¬¸ì„œ í–‰ë ¬ ìƒì„±\n",
    "# ----------------------------------\n",
    "def extract_topic_matrix(lda_model, corpus, num_topics):\n",
    "    topic_matrix = []\n",
    "    for doc in corpus:\n",
    "        dist = lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "        topic_matrix.append([prob for _, prob in sorted(dist)])\n",
    "    return pd.DataFrame(topic_matrix, columns=[f\"Topic_{i}\" for i in range(num_topics)])\n",
    "\n",
    "# ----------------------------------\n",
    "# ğŸ“Š ìš”ì¸ ë¶„ì„\n",
    "# ----------------------------------\n",
    "def run_factor_analysis(topic_df, n_factors=5, max_iter=500):\n",
    "    fa = FactorAnalysis(n_components=n_factors, random_state=42, max_iter=max_iter)\n",
    "    factors = fa.fit_transform(topic_df)\n",
    "    loadings = pd.DataFrame(fa.components_.T, index=topic_df.columns, columns=[f\"Factor_{i+1}\" for i in range(n_factors)])\n",
    "    return pd.DataFrame(factors, columns=loadings.columns), loadings\n",
    "\n",
    "# ----------------------------------\n",
    "# ğŸ“Š ìš”ì¸ ë¶„ì„ ìƒìœ„ ë¬¸ì„œ 5ê°œ ì €ì¥\n",
    "# ----------------------------------\n",
    "# ----------------------------------\n",
    "# ğŸ“Š ìš”ì¸ ë¶„ì„ ìƒìœ„ ë¬¸ì„œ 5ê°œ ì €ì¥ (500ê¸€ì ì œí•œ ì¶”ê°€)\n",
    "# ----------------------------------\n",
    "def top_docs_by_factor(factor_df, docs_df, top_n=5, output_path='top_documents_by_factor.txt', data_type='journal'):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for factor in factor_df.columns:\n",
    "            f.write(f\"\\nğŸ“Œ ìƒìœ„ ë¬¸ì„œ - {factor}\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\")\n",
    "            top_indices = factor_df[factor].nlargest(top_n).index\n",
    "\n",
    "            for i in top_indices:\n",
    "                row = docs_df.loc[i]\n",
    "\n",
    "                f.write(f\"ğŸ“† Date: {row.get('date', '')}\\n\")\n",
    "                f.write(f\"ğŸ“„ Title: {row.get('title', '')}\\n\")\n",
    "\n",
    "                if data_type == 'journal':\n",
    "                    content = row.get('abstract', '')\n",
    "                elif data_type == 'article':\n",
    "                    content = row.get('content', '')\n",
    "                else:\n",
    "                    raise ValueError(\"data_typeì€ 'journal' ë˜ëŠ” 'article' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "                # ğŸ”µ ë³¸ë¬¸ 500ê¸€ìê¹Œì§€ë§Œ ì €ì¥\n",
    "                if len(content) > 500:\n",
    "                    content = content[:500].rstrip() + \"...\"\n",
    "\n",
    "                f.write(f\"ğŸ” Content (500ì ì´ë‚´): {content}\\n\")\n",
    "                f.write(f\"ğŸ·ï¸ Keywords: {row.get('keywords', '')}\\n\")\n",
    "                f.write(\"-\"*60 + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ (ë³¸ë¬¸ 500ì ì œí•œ ì ìš©): {output_path}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# ğŸ“ LDA ëª¨ë¸ë¡œë¶€í„° í† í”½ë³„ í‚¤ì›Œë“œ ì €ì¥\n",
    "# ----------------------------------\n",
    "def save_lda_topics(lda_model, num_words, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for idx, topic in lda_model.show_topics(num_topics=-1, num_words=num_words, formatted=False):\n",
    "            keywords = \", \".join([word for word, _ in topic])\n",
    "            f.write(f\"Topic {idx}: {keywords}\\n\")\n",
    "    print(f\"âœ… LDA í† í”½ ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# ğŸ¯ ë…„ë„ë³„ë¡œ LDA + Factor ë¶„ì„\n",
    "# ----------------------------------\n",
    "def run_yearly_lda_factor_analysis(df, data_type='journal', n_topics=10, n_factors=5, vectorizer_method='tfidf', max_features=5000, output_dir='results'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    years = sorted(df['date'].unique())\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"ğŸ”µ Processing year: {year}\")\n",
    "\n",
    "        # í•´ë‹¹ ì—°ë„ ë°ì´í„° ì¶”ì¶œ\n",
    "        year_df = df[df['date'] == year].reset_index(drop=True)\n",
    "\n",
    "        # ë²¡í„°í™”\n",
    "        vectorizer, vectorized_matrix = preprocess_and_vectorize(year_df, method=vectorizer_method, max_features=max_features, data_type=data_type)\n",
    "\n",
    "        # Gensim LDAìš© corpus ì¤€ë¹„\n",
    "        processed_docs = prepare_documents(year_df, data_type=data_type)\n",
    "        dictionary = corpora.Dictionary(processed_docs)\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "        # LDA ëª¨ë¸ í•™ìŠµ\n",
    "        lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topics, random_state=42, passes=10)\n",
    "\n",
    "        # ğŸ¯ LDA í† í”½ë³„ í‚¤ì›Œë“œ ì €ì¥ ì¶”ê°€\n",
    "        save_lda_topics(\n",
    "            lda_model=lda_model,\n",
    "            num_words=10,  # í† í”½ë‹¹ ìƒìœ„ 10ê°œ ë‹¨ì–´\n",
    "            output_path=f\"{output_dir}/02_{data_type}_{year}_lda_topics.txt\"\n",
    "        )\n",
    "\n",
    "        # í† í”½-ë¬¸ì„œ í–‰ë ¬ ìƒì„±\n",
    "        topic_df = extract_topic_matrix(lda_model, corpus, n_topics)\n",
    "\n",
    "        # Factor Analysis\n",
    "        factor_df, loadings = run_factor_analysis(topic_df, n_factors=n_factors)\n",
    "\n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        topic_df.to_csv(f\"{output_dir}/02_{data_type}_{year}_topic_matrix.csv\", index=False)\n",
    "        factor_df.to_csv(f\"{output_dir}/02_{data_type}{year}_factor_scores.csv\", index=False)\n",
    "        loadings.to_csv(f\"{output_dir}/02_{data_type}{year}_factor_loadings.csv\", index=True)\n",
    "\n",
    "        top_docs_by_factor(\n",
    "            factor_df=factor_df,\n",
    "            docs_df=year_df,\n",
    "            top_n=5,\n",
    "            output_path=f\"{output_dir}/02_{data_type}_{year}_top_docs_by_factor.txt\",  # ì—¬ê¸° ìˆ˜ì •\n",
    "            data_type=data_type\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… Year {year} ì™„ë£Œ! (Topic Matrix, Factor Scores, Loadings, Top Docs ì €ì¥)\")\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T01:00:01.830824Z",
     "start_time": "2025-04-29T01:00:01.620503Z"
    }
   },
   "cell_type": "code",
   "source": "df=pd.read_csv('data/01_combined_article.csv')",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T01:07:35.583031Z",
     "start_time": "2025-04-29T01:00:01.893415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_yearly_lda_factor_analysis(\n",
    "    df,  # ğŸ‘‰ ë‹¹ì‹ ì´ ë§Œë“  ë°ì´í„°í”„ë ˆì„\n",
    "    data_type='article',  # ğŸ‘‰ ë…¼ë¬¸ì´ë©´ 'journal', ë‰´ìŠ¤ ê¸°ì‚¬ë©´ 'article'\n",
    "    n_topics=10,          # ğŸ‘‰ ê³ ì •: LDA í† í”½ ê°œìˆ˜\n",
    "    n_factors=5,          # ğŸ‘‰ ê³ ì •: Factor Analysis ìš”ì¸ ìˆ˜\n",
    "    vectorizer_method='tfidf',  # ğŸ‘‰ 'count'ë‚˜ 'tfidf' ì¤‘ ì„ íƒ ('tfidf' ì¶”ì²œ)\n",
    "    max_features=5000,     # ğŸ‘‰ ë²¡í„°ë¼ì´ì € ìµœëŒ€ ë‹¨ì–´ ìˆ˜ (ì„ íƒì‚¬í•­)\n",
    "    output_dir='data/result/02'   # ğŸ‘‰ ê²°ê³¼ ì €ì¥ í´ë”ëª…\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”µ Processing year: 2021\n",
      "âœ… LDA í† í”½ ì €ì¥ ì™„ë£Œ: data/result/02/02_article_2021_lda_topics.txt\n",
      "âœ… ì €ì¥ ì™„ë£Œ (ë³¸ë¬¸ 500ì ì œí•œ ì ìš©): data/result/02/02_article_2021_top_docs_by_factor.txt\n",
      "âœ… Year 2021 ì™„ë£Œ! (Topic Matrix, Factor Scores, Loadings, Top Docs ì €ì¥)\n",
      "ğŸ”µ Processing year: 2022\n",
      "âœ… LDA í† í”½ ì €ì¥ ì™„ë£Œ: data/result/02/02_article_2022_lda_topics.txt\n",
      "âœ… ì €ì¥ ì™„ë£Œ (ë³¸ë¬¸ 500ì ì œí•œ ì ìš©): data/result/02/02_article_2022_top_docs_by_factor.txt\n",
      "âœ… Year 2022 ì™„ë£Œ! (Topic Matrix, Factor Scores, Loadings, Top Docs ì €ì¥)\n",
      "ğŸ”µ Processing year: 2023\n",
      "âœ… LDA í† í”½ ì €ì¥ ì™„ë£Œ: data/result/02/02_article_2023_lda_topics.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_news/lib/python3.11/site-packages/sklearn/decomposition/_factor_analysis.py:296: ConvergenceWarning: FactorAnalysis did not converge. You might want to increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ (ë³¸ë¬¸ 500ì ì œí•œ ì ìš©): data/result/02/02_article_2023_top_docs_by_factor.txt\n",
      "âœ… Year 2023 ì™„ë£Œ! (Topic Matrix, Factor Scores, Loadings, Top Docs ì €ì¥)\n",
      "ğŸ”µ Processing year: 2024\n",
      "âœ… LDA í† í”½ ì €ì¥ ì™„ë£Œ: data/result/02/02_article_2024_lda_topics.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_news/lib/python3.11/site-packages/sklearn/decomposition/_factor_analysis.py:296: ConvergenceWarning: FactorAnalysis did not converge. You might want to increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ (ë³¸ë¬¸ 500ì ì œí•œ ì ìš©): data/result/02/02_article_2024_top_docs_by_factor.txt\n",
      "âœ… Year 2024 ì™„ë£Œ! (Topic Matrix, Factor Scores, Loadings, Top Docs ì €ì¥)\n",
      "ğŸ”µ Processing year: 2025\n",
      "âœ… LDA í† í”½ ì €ì¥ ì™„ë£Œ: data/result/02/02_article_2025_lda_topics.txt\n",
      "âœ… ì €ì¥ ì™„ë£Œ (ë³¸ë¬¸ 500ì ì œí•œ ì ìš©): data/result/02/02_article_2025_top_docs_by_factor.txt\n",
      "âœ… Year 2025 ì™„ë£Œ! (Topic Matrix, Factor Scores, Loadings, Top Docs ì €ì¥)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ë°ì´í„° ë¼ë²¨ë§ ì‘ì—… - í† í”½ëª¨ë¸ë§"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2021ë…„\n",
    "- Google Search: Topic 0: google, company, new, also, data, year, like, search, said, user\n",
    "- Facial Recognition: Topic 1: recognition, facial, system, uber, company, like, technology, driver, model, would\n",
    "- Customer Features: Topic 2: also, company, new, like, user, customer, feature, say, time, million\n",
    "- Startup: Topic 3: company, people, like, startup, tech, think, year, one, work, automation\n",
    "- Voice Technology: Topic 4: company, voice, tesla, like, also, system, say, technology, one, time\n",
    "- Platform: Topic 5: company, data, said, platform, startup, venture, customer, also, year, new\n",
    "- Machine Learning: Topic 6: data, model, company, learning, machine, robot, like, time, microsoft, one\n",
    "- Human-Technology Systems: Topic 7: data, company, like, research, also, human, google, work, system, say\n",
    "- Social Media: Topic 8: facebook, system, data, also, company, say, people, year, one, platform\n",
    "- Programming: Topic 9: codex, code, openai, food, company, people, one, video, like, say\n",
    "\n",
    "## 2022ë…„\n",
    "- Robotics: Topic 0: robot, company, google, robotics, like, system, new, year, also, one\n",
    "- Data-Driven Analysis: Topic 1: data, model, like, also, carbon, clearview, one, work, say, company\n",
    "- Language Models: Topic 2: language, model, data, google, company, like, say, system, new, tool\n",
    "- Startup: Topic 3: company, data, said, startup, also, like, technology, year, new, platform\n",
    "- Platform: Topic 4: data, company, nvidia, said, platform, also, new, time, say, year\n",
    "- Supply Chain: Topic 5: microsoft, data, image, skin, business, use, startup, supply, say, chain\n",
    "- Human-AI Interaction: Topic 6: mem, say, human, machine, company, like, musk, robot, people, one\n",
    "- Customer Service: Topic 7: company, startup, customer, said, data, platform, service, new, year, capital\n",
    "- Generative AI: Topic 8: image, model, like, system, text, art, video, diffusion, work, make\n",
    "- Human-Technology Systems: Topic 9: company, data, like, user, also, new, say, use, technology, startup\n",
    "\n",
    "## 2023ë…„\n",
    "- EMS: Topic 0: user, data, company, also, tool, like, new, microsoft, say, use\n",
    "- Chat GPT: Topic 1: openai, chatgpt, microsoft, model, gpt, copilot, company, user, like, text\n",
    "- Generative AI: Topic 2: model, data, company, generative, also, said, risk, open, source, like\n",
    "- Online Communities: Topic 3: think, like, thing, going, people, one, way, really, lot, get\n",
    "- OpenAI Governance: Topic 4: openai, altman, microsoft, board, company, ceo, said, new, team, sam\n",
    "- Google Search: Topic 5: google, new, search, like, tool, company, also, content, user, image\n",
    "- Search Engine: Topic 6: google, new, feature, microsoft, bing, like, also, user, search, app\n",
    "- Music Tools: Topic 7: music, company, artist, like, work, new, tool, year, one, song\n",
    "- Startup: Topic 8: company, model, generative, startup, image, data, like, said, new, customer\n",
    "- Voice Technology: Topic 9: voice, company, like, people, musk, one, said, time, would, say\n",
    "\n",
    "## 2024ë…„\n",
    "- Online Communities: Topic 0: think, people, going, thing, way, like, right, really, work, get\n",
    "- Startup: Topic 1: startup, data, company, model, said, year, also, new, million, generative\n",
    "- Product: Topic 2: model, like, company, one, customer, product, data, said, time, say\n",
    "- Apple: Topic 3: apple, image, video, new, feature, like, app, also, user, device\n",
    "- Sustainability Tech: Topic 4: arm, pin, waste, meeting, rabbit, humane, recall, otter, greyparrot, recycling\n",
    "- Google Gemini: Topic 5: google, gemini, search, feature, user, new, model, like, also, generative\n",
    "- Enterprise AI: Topic 6: model, microsoft, openai, copilot, data, gpt, open, company, training, developer\n",
    "- Tech Personalities: Topic 7: chromebook, velastegui, zuckerberg, supermaven, superannotate, fandom, wohl, cable, defcon, wendy\n",
    "- Big Tech Company: Topic 8: openai, meta, chatgpt, company, also, safety, board, said, altman, content\n",
    "- Content Platform: Topic 9: content, like, voice, company, one, people, said, user, news, use\n",
    "\n",
    "## 2025ë…„\n",
    "- Online Communities: Topic 0: think, going, like, people, company, make, want, one, get, way\n",
    "- Chatbot: Topic 1: model, openai, google, gemini, chatgpt, company, user, deepseek, like, new\n",
    "- Language Models: Topic 2: model, company, grok, said, data, think, also, like, product, language\n",
    "- Tech Personalities: Topic 3: company, google, openai, musk, said, billion, trump, startup, also, tech\n",
    "- AI Search: Topic 4: data, google, perplexity, company, also, say, said, search, use, startup\n",
    "- Hardware AI: Topic 5: nvidia, tesla, rtx, company, musk, blackwell, also, xai, chip, new\n",
    "- Enterprise AI: Topic 6: microsoft, model, meta, company, data, openai, copilot, chatgpt, new, user\n",
    "- AI Agents: Topic 7: openai, agent, company, alexa, amazon, said, new, like, system, also\n",
    "- Apple: Topic 8: apple, feature, new, device, intelligence, also, company, model, game, humane\n",
    "- Multimedia Tech: Topic 9: one, like, video, rtx, think, new, thing, google, want, get"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:35:17.896458Z",
     "start_time": "2025-05-02T03:35:16.234172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ì—°ë„ë³„ í† í”½ ë ˆì´ë¸” ë”•ì…”ë„ˆë¦¬\n",
    "topic_label_mappings = {\n",
    "    2021: {\n",
    "        0: \"Google Search\",\n",
    "        1: \"Facial Recognition\",\n",
    "        2: \"Customer Features\",\n",
    "        3: \"Startup\",\n",
    "        4: \"Voice Technology\",\n",
    "        5: \"Platform\",\n",
    "        6: \"Machine Learning\",\n",
    "        7: \"Human-Technology Systems\",\n",
    "        8: \"Social Media\",\n",
    "        9: \"Programming\"\n",
    "    },\n",
    "    2022: {\n",
    "        0: \"Robotics\",\n",
    "        1: \"Data-Driven Analysis\",\n",
    "        2: \"Language Models\",\n",
    "        3: \"Startup\",\n",
    "        4: \"Platform\",\n",
    "        5: \"Supply Chain\",\n",
    "        6: \"Human-AI Interaction\",\n",
    "        7: \"Customer Service\",\n",
    "        8: \"Generative AI\",\n",
    "        9: \"Human-Technology Systems\"\n",
    "    },\n",
    "    2023: {\n",
    "        0: \"EMS\",\n",
    "        1: \"Chat GPT\",\n",
    "        2: \"Generative AI\",\n",
    "        3: \"Online Communities\",\n",
    "        4: \"OpenAI Governance\",\n",
    "        5: \"Google Search\",\n",
    "        6: \"Search Engine\",\n",
    "        7: \"Music Tools\",\n",
    "        8: \"Startup\",\n",
    "        9: \"Voice Technology\"\n",
    "    },\n",
    "    2024: {\n",
    "        0: \"Online Communities\",\n",
    "        1: \"Startup\",\n",
    "        2: \"Product\",\n",
    "        3: \"Apple\",\n",
    "        4: \"Sustainability Tech\",\n",
    "        5: \"Google Gemini\",\n",
    "        6: \"Enterprise AI\",\n",
    "        7: \"Tech Personalities\",\n",
    "        8: \"Big Tech Company\",\n",
    "        9: \"Content Platform\"\n",
    "    },\n",
    "    2025: {\n",
    "        0: \"Online Communities\",\n",
    "        1: \"Chatbot\",\n",
    "        2: \"Language Models\",\n",
    "        3: \"Tech Personalities\",\n",
    "        4: \"AI Search\",\n",
    "        5: \"Hardware AI\",\n",
    "        6: \"Enterprise AI\",\n",
    "        7: \"AI Agents\",\n",
    "        8: \"Apple\",\n",
    "        9: \"Multimedia Tech\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ëŒ€ìƒ ì—°ë„ ë¦¬ìŠ¤íŠ¸\n",
    "years = [2021, 2022, 2023, 2024, 2025]\n",
    "\n",
    "# ì—°ë„ë³„ ì²˜ë¦¬\n",
    "for year in years:\n",
    "    print(f\"ğŸ”„ Processing year: {year}\")\n",
    "\n",
    "    # 1. ê¸°ì‚¬ ë°ì´í„° ë¡œë“œ ë° í•„í„°ë§\n",
    "    article_df = pd.read_csv('data/01_combined_article.csv')\n",
    "    article_df = article_df[article_df['date'] == year].reset_index(drop=True)\n",
    "\n",
    "    # 2. í•´ë‹¹ ì—°ë„ í† í”½ í–‰ë ¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    topic_matrix_path = f'data/result/02/02_article_{year}_topic_matrix.csv'\n",
    "    topic_matrix = pd.read_csv(topic_matrix_path)\n",
    "\n",
    "    # 3. ê°€ì¥ ë†’ì€ í† í”½ ì„ íƒ\n",
    "    top_topic = topic_matrix.idxmax(axis=1)\n",
    "    top_topic_num = top_topic.str.extract(r'(\\d+)').astype(int)[0]\n",
    "\n",
    "    # 4. í† í”½ ë¼ë²¨ ë§¤í•‘\n",
    "    topic_label_mapping = topic_label_mappings[year]\n",
    "    article_df['topic_label'] = top_topic_num.map(topic_label_mapping)\n",
    "\n",
    "    # 5. ì €ì¥\n",
    "    save_path = f'data/result/02/02_article_{year}_labeled.csv'\n",
    "    article_df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… Saved labeled file for {year} to: {save_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing year: 2021\n",
      "âœ… Saved labeled file for 2021 to: data/result/02/02_article_2021_labeled.csv\n",
      "ğŸ”„ Processing year: 2022\n",
      "âœ… Saved labeled file for 2022 to: data/result/02/02_article_2022_labeled.csv\n",
      "ğŸ”„ Processing year: 2023\n",
      "âœ… Saved labeled file for 2023 to: data/result/02/02_article_2023_labeled.csv\n",
      "ğŸ”„ Processing year: 2024\n",
      "âœ… Saved labeled file for 2024 to: data/result/02/02_article_2024_labeled.csv\n",
      "ğŸ”„ Processing year: 2025\n",
      "âœ… Saved labeled file for 2025 to: data/result/02/02_article_2025_labeled.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ë°ì´í„° ë¼ë²¨ë§ ì‘ì—… - Factor Analysis"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2021ë…„\n",
    "- Factor_1: IS ë°œì „ : NLP, GPT-3, ìë™ ì™„ì„±, ì½”ë“œ ìƒì„±, ë”¥ëŸ¬ë‹, ë¡œë´‡ ì„¤ê³„ ë“± ê¸°ìˆ ì  íŠ¹ì„±ê³¼ ì‹œìŠ¤í…œ ì¤‘ì‹¬ AI ë°œì „ ë‚´ìš©ì„ ë‹¤ë£¸. ëŒ€í‘œ ë¬¸ì„œë“¤ì€ ëŒ€ë¶€ë¶„ ëª¨ë¸ë§, MLOps, ì‹œìŠ¤í…œ êµ¬ì¡°, ì–¸ì–´ ì²˜ë¦¬ ë“± ê¸°ìˆ  êµ¬í˜„ ì¤‘ì‹¬.\n",
    "- Factor_2: IT ì¡°ì§ : ì¡°ì§ ì°¨ì›ì˜ AI í™œìš©, ë¬¼ë¥˜ ìë™í™”, ëª¨ë¹Œë¦¬í‹° ìƒíƒœê³„ ê¸°íš, ê¸°ì—… ë³´ì•ˆê³¼ ì¸ìˆ˜í•©ë³‘ ë“±ì˜ ì „ëµì  ì˜ì‚¬ê²°ì • ì´ìŠˆì— ì´ˆì . ì˜ˆ: ë¬¼ë¥˜ ìŠ¤íƒ€íŠ¸ì—… ì„±ì¥, ì •ë¶€-ê¸°ì—… ê°„ êµí†µê³„íš, ì‚¬ì´ë²„ë³´ì•ˆ ê¸°ì—… ì„±ì¥ ì „ëµ.\n",
    "- Factor_3: IT ì‹œì¥ : ì•”í˜¸í™”í ìƒíƒœê³„, ìŠ¤í¬ì¸  ìŠ¤íŠ¸ë¦¬ë°, í”„ë¼ì´ë²„ì‹œë¥¼ ê°•ì¡°í•œ í”Œë«í¼, ê´‘ê³  ìƒíƒœê³„ ë“± ì‹œì¥ ë° ì‚°ì—… êµ¬ì¡° ë³€í™”ì™€ ê´€ë ¨ëœ ì£¼ì œ. ì˜ˆ: Tether ë…¼ë€, Apple í”„ë¼ì´ë²„ì‹œ ì •ì±…, Instagram ì²­ì†Œë…„ ì •ì±… ë“±.\n",
    "- Factor_4: IT ê°œì¸ : ê°œì¸ì˜ í–‰ë™, ì¸ì‹, ê°ì •ê³¼ ê´€ë ¨ëœ ê¸°ìˆ ì˜ ì˜í–¥ ë¶„ì„. ì˜ˆ: Babylon Healthì˜ ìœ¤ë¦¬ ë…¼ë€, í”„ë¼ì´ë²„ì‹œ ê·œì œ, Facebook ë‚´ë¶€ê³ ë°œì ì´ìŠˆ ë“± ê°œì¸ì˜ ê¸°ìˆ  ìˆ˜ìš©ì„±ê³¼ ê·œì œ ë°˜ì‘ ì¤‘ì‹¬.\n",
    "- Factor_5: IT ê·¸ë£¹ : ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„ì™€ ê³µë™ì²´, íˆ¬ìì-ì°½ì—…ì ë„¤íŠ¸ì›Œí¬, ê¸°ì—… ê°„ í˜‘ì—… ìƒíƒœê³„ ë“± ì§‘ë‹¨ ë‹¨ìœ„ì˜ ìƒí˜¸ì‘ìš©ê³¼ ì»¤ë®¤ë‹ˆí‹° í™•ì¥ ê´€ë ¨ ì£¼ì œ. ì˜ˆ: ë„ì‹œë³„ í…Œí¬ í—ˆë¸Œ ë¶„ì„, íˆ¬ìì ë™í–¥, í˜‘ì—… ê¸°ë°˜ í˜ì‹  ì‚¬ë¡€ ë“±.\n",
    "\n",
    "## 2022ë…„\n",
    "- Factor_1: IT ì¡°ì§ : AI ê¸°ì—…ì˜ ë§ˆì¼€íŒ… ì „ëµ, ë¡œë³´íƒì‹œ ìƒìš©í™”, ì‚°ì—…ìš© ë“œë¡ , ì‚¬ì´ë²„ ë³´ì•ˆ ë“± â€˜ì¡°ì§ ìˆ˜ì¤€ì˜ AI ê¸°ìˆ  ì ìš©, ì œí’ˆí™”, ë‚´ë¶€ ì—­ëŸ‰ êµ¬ì¶•â€™ì— ì´ˆì . ê¸°ìˆ ì„ ì¡°ì§ ë‚´ë¶€ ì—­ëŸ‰ê³¼ ì—°ê²°ì§“ê±°ë‚˜ ê¸°ì—… ì „ëµì˜ ì¼ë¶€ë¡œ ë‹¤ë£¸.\n",
    "- Factor_2: IT ê°œì¸ : ChatGPT ì‚¬ìš©ì„±, ê°œì¸í™” ì˜ë£Œ í”Œë«í¼, ì†Œë¹„ì ë§ì¶¤í˜• ì‡¼í•‘, ì–¸ì–´ ê²€ìƒ‰ íŒê²° ì‚¬ë¡€ ë“± â€˜ê°œì¸ ê²½í—˜ê³¼ ë°˜ì‘, ì¸ê°„-AI ìƒí˜¸ì‘ìš©â€™ì— ì´ˆì ì„ ë‘” ë¬¸ì„œê°€ ë‹¤ìˆ˜. íŠ¹íˆ ì˜ë£Œ, ì†Œë¹„ì í–‰ë™, ì •ì±…ì  ë°˜ì‘ì„ ì¤‘ì‹¬ìœ¼ë¡œ ê°œì¸ì˜ ê¸°ìˆ  ìˆ˜ìš©ì„± íƒìƒ‰.\n",
    "- Factor_3: IS ë°œì „ : ChatGPT ê¸°ìˆ  ìì²´, ì–¸ì–´ëª¨ë¸ ì•ˆì „ì„±, ë°©ì†¡Â·ë¯¸ë””ì–´ í˜ì‹  ë“± â€˜AI ê¸°ìˆ , ì–¸ì–´ëª¨ë¸, ê°œì¸í™” ì•Œê³ ë¦¬ì¦˜ì˜ ì§„ë³´ì™€ ì‘ìš©â€™ì„ ë‹¤ë£¸. ê¸°ìˆ  êµ¬ì¡°, ì•Œê³ ë¦¬ì¦˜, ì‹œìŠ¤í…œ ì„¤ê³„ì™€ ê°™ì€ IS ê¸°ìˆ  ë°œì „ì˜ ì •ì²´ì„±ê³¼ ë¶€í•©.\n",
    "- Factor_4: IT ì‹œì¥ : ë‰´ìŠ¤ ë¯¸ë””ì–´ì˜ web3 ì „í™˜, ë©”íƒ€ì˜ ë‹¤êµ­ì–´ ë²ˆì—­, ì–¸ë¡ ì˜ í˜ì‹  ë¶€ì¡± ë“± â€˜AIì™€ ì‚¬íšŒ, ì‚°ì—…, ë¯¸ë””ì–´ ì‹œìŠ¤í…œ ê°„ ê´€ê³„ ë° ì •ì±… ì˜í–¥â€™ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‚°ì—…/ì •ì±… êµ¬ì¡°ì™€ ì‹œì¥ ë³€í™”ì— ì´ˆì .\n",
    "- Factor_5: IT ê·¸ë£¹ : ìƒì„±í˜• AI ì´ë¯¸ì§€Â·ì˜ìƒ íˆ´ì´ ì¼ë°˜ ì‚¬ìš©ì ì»¤ë®¤ë‹ˆí‹°ì™€ ì°½ì‘ì ê·¸ë£¹ ë‚´ì—ì„œ ìƒì„±Â·ìœ í†µë˜ë©° ë§Œë“¤ì–´ë‚´ëŠ” ì§‘ë‹¨ì  ì°½ì‘ ê²½í—˜ì— ëŒ€í•œ ë…¼ì˜. AI ê¸°ë°˜ ì˜ˆìˆ  ë„êµ¬ì˜ í™•ì‚°ì´ í˜‘ì—…ê³¼ ì§‘ë‹¨ ì°½ì‘ ìƒíƒœê³„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ê°•ì¡°.\n",
    "\n",
    "## 2023ë…„\n",
    "- Factor_1: IT ì¡°ì§ : Adobe, OpenAI, EvenUp, CoreWeave ë“± ê¸°ì—… ì¤‘ì‹¬ì˜ ê¸°ìˆ  ê°œë°œê³¼ ì „ëµì  ì‹¤í–‰ì„ ë‹¤ë£¬ ê¸°ì‚¬ë“¤ì´ ì£¼ë¥¼ ì´ë£¸. ì˜ˆ: Firefly ìƒì„±í˜• ëª¨ë¸ ì¶œì‹œ, ë²•ë¥  ìë™í™”, í´ë¼ìš°ë“œ ì¸í”„ë¼ ì „í™˜ ë“± ì¡°ì§ì˜ IT í™œìš©ê³¼ ê¸°ìˆ  ë‚´ì¬í™” ì „ëµì´ í•µì‹¬.\n",
    "- Factor_2: IT ì‹œì¥ : ìœ ëŸ½ì—°í•© AI ë²•ì•ˆ, ìº˜ë¦¬í¬ë‹ˆì•„ AI ê·œì œ ë“± ê±°ë²„ë„ŒìŠ¤ì™€ ì‹œì¥ ê·œì¹™ ì„¤ì •ì„ ë‘˜ëŸ¬ì‹¼ ì •ì±…, ì‚°ì—… ë™í–¥ ê¸°ì‚¬ ë‹¤ìˆ˜. ì´ëŠ” ì‹œì¥ êµ¬ì¡° ë³€í™”ì™€ ì‚°ì—… ê°„ ìƒí˜¸ì‘ìš©ì´ë¼ëŠ” 'IT ì‹œì¥'ì˜ ì •ì˜ì™€ ë¶€í•©.\n",
    "- Factor_3: IT ê°œì¸ : AI ê¸°ìˆ ì˜ ì‚¬íšŒì  ìˆ˜ìš©, ChatGPTì˜ ì‚¬ìš©ì ì˜í–¥, API í™œìš© ì‚¬ë¡€ ë“± ê°œì¸ì˜ í–‰ë™Â·ì¸ì§€Â·ê²½í—˜ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë‚´ìš©. ê°œì¸í™”ëœ ìƒí˜¸ì‘ìš© ë° ê¸°ìˆ  ìˆ˜ìš©ì„± ë¬¸ì œë¥¼ ë‹¤ë£¸.\n",
    "- Factor_4: IS ë°œì „ : GPT-4, GPT-4 Turbo ë“± ë©€í‹°ëª¨ë‹¬ ì–¸ì–´ëª¨ë¸ì˜ ê¸°ìˆ ì  ì§„ë³´, API ì¶œì‹œ, ëª¨ë¸ ì„±ëŠ¥ ë…¼ì˜ ë“± ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë° ê¸°ëŠ¥ì„± ì¤‘ì‹¬ì˜ ê¸°ìˆ  ë°œì „ ê¸°ì‚¬ ì¤‘ì‹¬.\n",
    "- Factor_5: IT ê·¸ë£¹ : Clearview AI, Squarespace, Harvard êµìˆ˜, ìŒì•… ì‚°ì—… CEO ë“±ì˜ íŒŸìºìŠ¤íŠ¸ ë° ëŒ€ë‹´ ì¤‘ì‹¬ ê¸°ì‚¬ë¡œ, í˜‘ì—… ì»¤ë®¤ë‹ˆí‹°, ìœ¤ë¦¬, ì €ì‘ê¶Œ, ì°½ì‘ ì§‘ë‹¨ ë“± ë‹¤ì–‘í•œ ì§‘ë‹¨ì˜ ì—­í• ê³¼ ìƒí˜¸ì‘ìš©ì„ ì¡°ëª….\n",
    "\n",
    "## 2024ë…„\n",
    "- Factor_1: IT ì¡°ì§ : Apple, Spotify, Amazon, Adobe ë“± ëŒ€í˜• ê¸°ìˆ  ê¸°ì—…ë“¤ì´ ìì‚¬ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ì— AI ê¸°ëŠ¥ì„ ì „ëµì ìœ¼ë¡œ ë„ì…í•œ ì‚¬ë¡€ê°€ ì¤‘ì‹¬. ì˜ˆ: WWDCì—ì„œ ë°œí‘œí•œ Appleì˜ AI ê³„íš, Adobeì˜ Firefly, Amazonê³¼ Spotifyì˜ AI ì¶”ì²œ ì‹œìŠ¤í…œ ë„ì… ë“±ì€ ì¡°ì§ ì°¨ì›ì˜ ê¸°ìˆ  ë‚´ì¬í™”ì™€ ì‚¬ì—… ì „ëµìœ¼ë¡œ í•´ì„ë¨.\n",
    "- Factor_2: IS ë°œì „ : DevSecOps íŠ¸ë Œë“œ, ìˆ˜ë¦¬ ì§„ë‹¨ì„ ìœ„í•œ ìŒí–¥ ê¸°ë°˜ AI, ì‹ ì•½ ì„¤ê³„, íŠ¹í—ˆ ë¶„ì„ í”Œë«í¼ ë“±ì€ ëª¨ë‘ ì•Œê³ ë¦¬ì¦˜Â·í•˜ë“œì›¨ì–´Â·ë°ì´í„° ê¸°ë°˜ ì‹œìŠ¤í…œ í˜ì‹  ì‚¬ë¡€. ì´ëŠ” ê¸°ìˆ ì  êµ¬ì¡°ì™€ ì„±ëŠ¥ í–¥ìƒì´ë¼ëŠ” ì „í†µì  IS ë°œì „ì˜ ì •ì˜ì— ë¶€í•©í•¨.\n",
    "- Factor_3: IT ì‹œì¥ : Metaì˜ Llama 3, Mistral AI, xAI, AI2ì˜ ì˜¤í”ˆëª¨ë¸ ê³µê°œ ë“±ì€ AI ëª¨ë¸ ê²½ìŸ êµ¬ë„ ì†ì—ì„œì˜ ì‹œì¥ ì£¼ë„ê¶Œ í™•ë³´ ì „ëµ. ì˜¤í”ˆì†ŒìŠ¤ ìƒíƒœê³„ í˜•ì„±ê³¼ ê¸°ìˆ  ë¦¬ë”ì‹­ ê²½ìŸì€ ì‚°ì—… ë° í”Œë«í¼ ì‹œì¥ì˜ ì—­í•™ì„ ë°˜ì˜í•¨.\n",
    "- Factor_4: IT ê°œì¸ : Google Gemini ê¸°ë°˜ í¬ë¡¬ ë¸Œë¼ìš°ì € ê¸°ëŠ¥, AI ë©€í‹°ì„œì¹˜, ê°œì¸ ë§ì¶¤í˜• ê¸°ëŠ¥ í†µí•© ë“±ì€ ì‚¬ìš©ìì˜ ì›¹ ê²½í—˜ì„ ì§ì ‘ì ìœ¼ë¡œ ë³€í™”ì‹œí‚¤ëŠ” ê¸°ìˆ . ì´ëŠ” ê°œì¸ì˜ ìˆ˜ìš©ì„±, ì‚¬ìš©ì„±, UX ë³€í™” ë“± â€˜ITì™€ ê°œì¸ì˜ ìƒí˜¸ì‘ìš©â€™ ì¤‘ì‹¬.\n",
    "- Factor_5: IT ê·¸ë£¹ : ì •ì¹˜ ë”¥í˜ì´í¬, ìœ ëª…ì¸ ìƒì„± ì½˜í…ì¸  ë…¼ë€, íŒ¬ë¤ ì§‘ë‹¨ì˜ ëŒ€ì‘, ì•„ë™ ë³´í˜¸ ê´€ë ¨ ê·œì œ ë“±ì€ ëª¨ë‘ ì§‘ë‹¨ì  ëŒ€ì‘, ì‚¬íšŒ ì§‘í•©ì²´ ë‚´ ê·œë²”/ì‹ ë¢°/í˜‘ë ¥ êµ¬ì¡°ì™€ ê´€ë ¨. AIê°€ ì§‘ë‹¨ ê°„ ìƒí˜¸ì‘ìš© ë° ì‚¬íšŒ ì§ˆì„œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„ìœ¼ë¡œ ë¶„ë¥˜ë¨.\n",
    "\n",
    "## 2025ë…„\n",
    "- Factor_1: IS ë°œì „ : Google Gemini, GPT-4.5, DeepSeek ë“± ì£¼ìš” ìƒì„±í˜• AI ëª¨ë¸ê³¼ ê¸°ìˆ  ìŠ¤íƒì˜ ë¹„êµ, êµ¬ì¡°, í›ˆë ¨ ë°©ì‹ ë“±ì— ê´€í•œ ê¸°ì‚¬ ì¤‘ì‹¬. ëª¨ë¸ ì„±ëŠ¥, ê¸°ëŠ¥ ê°œì„ , AI í›ˆë ¨ ë°ì´í„° ë“± ê¸°ìˆ ì  ì‹œìŠ¤í…œ ë°œì „ì— ì´ˆì ì„ ë‘ê³  ìˆì–´ IS ë°œì „ìœ¼ë¡œ ë¶„ë¥˜ë¨.\n",
    "- Factor_2: IT ì¡°ì§ : Metaì˜ ì €ì‘ê¶Œ ì†Œì†¡, ê¸°ì—… ë‚´ë¶€ ê²°ì • ê³¼ì •(ì˜ˆ: Zuckerbergì˜ LLaMA í›ˆë ¨ ìŠ¹ì¸), Fiverrì˜ ì—…ë¬´ ìë™í™” ì „ëµ, ì•„ë™ ì˜¨ë¼ì¸ ì•ˆì „ì„ ìœ„í•œ ì¡°ì§ ê°„ í˜‘ì—… ë“±. ì¡°ì§ ìˆ˜ì¤€ì—ì„œì˜ ì˜ì‚¬ê²°ì •, ì „ëµ ìˆ˜ë¦½, ë²•ì  ëŒ€ì‘ì´ ì¤‘ì‹¬.\n",
    "- Factor_3: IT ì‹œì¥ : ìœ ëŸ½ì˜ ì˜¤í”ˆì†ŒìŠ¤ LLM ì „ëµ, ê³µê³µ ì´ìµ ê¸°ë°˜ AI ìƒíƒœê³„ êµ¬ì¶•, Gemma 3ì˜ ë¼ì´ì„ ìŠ¤ ë¬¸ì œ, EU AI ë²•ì•ˆ ë“± ì‹œì¥Â·ì‚°ì—… ë‹¨ìœ„ì—ì„œì˜ ê±°ë²„ë„ŒìŠ¤, ê²½ìŸ, ê·œì œ, ìƒíƒœê³„ êµ¬ì„± ë…¼ì˜ë¡œ ì‹œì¥ ê´€ì ì— ì í•©.\n",
    "- Factor_4: IT ê°œì¸ : AI ì•ˆì „ì„± ë…¼ìŸ(AI doom), ì •ì¹˜ ë° ê¸°ìˆ  ê·œì œì— ëŒ€í•œ ì—¬ë¡  ë°˜ì‘, ëŒ€í˜• ê¸°ì—… ì¸ìˆ˜ì— ë”°ë¥¸ ì‚¬ìš©ì ë° ì‚¬íšŒì  ìˆ˜ìš©, íˆ¬ì ì‹¬ë¦¬ ë“± ê°œì¸ ìˆ˜ì¤€ì—ì„œ ê¸°ìˆ  ë³€í™”ì— ëŒ€í•œ ì¸ì§€Â·í–‰ë™Â·ê°ì • ë°˜ì‘ ì¤‘ì‹¬.\n",
    "- Factor_5: IT ê·¸ë£¹ : Splice, Dow Jones, UiPath, Omiì™€ ê°™ì€ ë‹¤ì–‘í•œ ì‚°ì—… ë‚´ ì¡°ì§ ë¦¬ë”ì™€ ê¸°ìˆ  ì°½ì—…ìë“¤ì˜ AI ë„ì…Â·ìœ¤ë¦¬Â·í‘œí˜„ì˜ ììœ  ë“±ì— ê´€í•œ ì¸í„°ë·° ì¤‘ì‹¬. ì´ëŠ” ê¸°ì—… ë‚´ë¶€ ì§‘ë‹¨, ì»¤ë®¤ë‹ˆí‹°, ì´í•´ê´€ê³„ì ì§‘ë‹¨ ê°„ ìƒí˜¸ì‘ìš©ê³¼ ì‹ ë¢° ì´ìŠˆë¥¼ ë°˜ì˜í•˜ì—¬ ê·¸ë£¹ ë‹¨ìœ„ì— í•´ë‹¹.\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T02:34:24.359481Z",
     "start_time": "2025-05-03T02:34:24.355382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "factor_label_mappings = {\n",
    "    2021: {\n",
    "        'Factor_1': 'IS ë°œì „',\n",
    "        'Factor_2': 'IT ì¡°ì§',\n",
    "        'Factor_3': 'IT ì‹œì¥',\n",
    "        'Factor_4': 'IT ê°œì¸',\n",
    "        'Factor_5': 'IT ê·¸ë£¹'\n",
    "    },\n",
    "    2022: {\n",
    "        'Factor_1': 'IT ì¡°ì§',\n",
    "        'Factor_2': 'IT ê°œì¸',\n",
    "        'Factor_3': 'IS ë°œì „',\n",
    "        'Factor_4': 'IT ì‹œì¥',\n",
    "        'Factor_5': 'IT ê·¸ë£¹'\n",
    "    },\n",
    "    2023: {\n",
    "        'Factor_1': 'IT ì¡°ì§',\n",
    "        'Factor_2': 'IT ì‹œì¥',\n",
    "        'Factor_3': 'IT ê°œì¸',\n",
    "        'Factor_4': 'IS ë°œì „',\n",
    "        'Factor_5': 'IT ê·¸ë£¹'\n",
    "    },\n",
    "    2024: {\n",
    "        'Factor_1': 'IT ì¡°ì§',\n",
    "        'Factor_2': 'IS ë°œì „',\n",
    "        'Factor_3': 'IT ì‹œì¥',\n",
    "        'Factor_4': 'IT ê°œì¸',\n",
    "        'Factor_5': 'IT ê·¸ë£¹'\n",
    "    },\n",
    "    2025: {\n",
    "        'Factor_1': 'IS ë°œì „',\n",
    "        'Factor_2': 'IT ì¡°ì§',\n",
    "        'Factor_3': 'IT ì‹œì¥',\n",
    "        'Factor_4': 'IT ê°œì¸',\n",
    "        'Factor_5': 'IT ê·¸ë£¹'\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T02:37:23.913751Z",
     "start_time": "2025-05-03T02:37:23.842186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "year = 2021\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "labeled_path = f'data/result/02/02_article_{year}_labeled.csv'\n",
    "factor_scores_path = f'data/result/02/02_article{year}_factor_scores.csv'\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "labeled_df = pd.read_csv(labeled_path)\n",
    "factor_scores = pd.read_csv(factor_scores_path)\n",
    "\n",
    "# ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ Factor ì„ íƒ ë° ë¼ë²¨ ë§¤í•‘\n",
    "factor_columns = ['Factor_1', 'Factor_2', 'Factor_3', 'Factor_4', 'Factor_5']\n",
    "factor_scores['max_factor'] = factor_scores[factor_columns].idxmax(axis=1)\n",
    "factor_scores['assigned_label'] = factor_scores['max_factor'].map(factor_label_mappings[year])  # âœ… ìˆ˜ì •\n",
    "\n",
    "# ë³‘í•©\n",
    "merged_df = pd.concat([\n",
    "    labeled_df.reset_index(drop=True),\n",
    "    factor_scores[['max_factor', 'assigned_label']].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# ì €ì¥\n",
    "merged_df.to_csv(f'data/result/02/02_article_{year}_merged_labeled.csv', index=False)\n",
    "print(merged_df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  date  \\\n",
      "0  Walmart is expanding its robot-powered fulfill...  2021   \n",
      "1    Google is investigating another top AI ethicist  2021   \n",
      "2  Amazon opens Alexa AI tech for the first time ...  2021   \n",
      "3  FTC settles with photo storage app that pivote...  2021   \n",
      "4                   Why Googleâ€™s union is a big deal  2021   \n",
      "\n",
      "                                             content  \\\n",
      "0  Walmart is planning to increase the number of ...   \n",
      "1  Google is investigating artificial intelligenc...   \n",
      "2  Amazon will now allow third-party companies th...   \n",
      "3  The Federal Trade Commission has reached a set...   \n",
      "4  On January 4th, 2021, Google workers and contr...   \n",
      "\n",
      "                                            keywords affiliations  \\\n",
      "0                    AI, Business, News, Robot, Tech        verge   \n",
      "1                             AI, Google, News, Tech        verge   \n",
      "2  AI, Amazon, Amazon Alexa, Business, Cars, News...        verge   \n",
      "3                    AI, News, Policy, Privacy, Tech        verge   \n",
      "4                  AI, Featured Videos, Google, Tech        verge   \n",
      "\n",
      "          topic_label max_factor assigned_label  \n",
      "0            Platform   Factor_2          IT ì¡°ì§  \n",
      "1       Google Search   Factor_4          IT ê°œì¸  \n",
      "2       Google Search   Factor_1          IS ë°œì „  \n",
      "3  Facial Recognition   Factor_5          IT ê·¸ë£¹  \n",
      "4       Google Search   Factor_4          IT ê°œì¸  \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T02:37:27.140519Z",
     "start_time": "2025-05-03T02:37:26.621537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë³‘í•©í•  íŒŒì¼ë“¤ì˜ ë…„ë„ ë¦¬ìŠ¤íŠ¸\n",
    "years = [2021, 2022, 2023, 2024, 2025]\n",
    "\n",
    "# ëª¨ë“  íŒŒì¼ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
    "dfs = []\n",
    "\n",
    "# ê° ë…„ë„ë³„ íŒŒì¼ì„ ì½ì–´ì„œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "for year in years:\n",
    "    file_path = f'data/result/02/02_article_{year}_merged_labeled.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['year'] = year  # í˜¹ì‹œ year ì»¬ëŸ¼ ì—†ìœ¼ë©´ ì¶”ê°€\n",
    "    dfs.append(df)\n",
    "\n",
    "# ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "merged_df.to_csv('data/result/02/02_article_2021_2025_merged_labeled.csv', index=False)\n",
    "\n",
    "print('âœ… ì €ì¥ ì™„ë£Œ: data/result/02/02_article_2021_2025_merged_labeled.csv')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: data/result/02/02_article_2021_2025_merged_labeled.csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
