import os
import re
import time
import requests
import pandas as pd
from bs4 import BeautifulSoup
from tqdm import tqdm

# ì—°ë„ ì„¤ì •
# ==========================
year = 2025
start_page = 71      # âœ… ì‹œì‘ í˜ì´ì§€ ì§€ì •
end_page = 80        # âœ… ì¢…ë£Œ í˜ì´ì§€ ì§€ì • (ì›í•˜ëŠ” ë§Œí¼ ìˆ˜ì •)
# ==========================

# ì—°ë„ë³„ scope_id ë§¤í•‘
scope_dict = {
    2021: '8db05028-a838-4e0f-911b-4ea544253c64',
    2022: '32d42543-f8b4-45fb-8c50-11a42cb8fe9a',
    2023: 'f36bf371-28c7-4427-bff7-718d2c995872',
    2024: '9d4848f6-1815-43f1-bcb5-be17e430d153',
    2025: 'c2af410a-9d5f-4f99-ad4e-626911b4e900'
}

scope_id = scope_dict.get(year)
if not scope_id:
    raise ValueError(f"âŒ ì—°ë„ {year}ì— ëŒ€í•œ scope_idê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

# ê¸°ë³¸ ì„¤ì •
BASE_URL = 'https://scholarspace.manoa.hawaii.edu'
BROWSE_BASE = f'{BASE_URL}/browse/dateissued?scope={scope_id}&bbm.page='
API_BASE = f'{BASE_URL}/server/api/core'
SAVE_DIR = f'HICS{year}_papers'
CSV_FILE = f'HICS{year}_metadata.csv'

def extract_paper_links_from_page(page_num: int) -> list:
    response = requests.get(f"{BROWSE_BASE}{page_num}")
    soup = BeautifulSoup(response.text, 'html.parser')
    a_tags = soup.select('a[href^="/items/"]')
    return [BASE_URL + a['href'] for a in a_tags if a.get('href')]

def extract_paper_metadata(url: str, save_dir: str = SAVE_DIR) -> pd.DataFrame:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    title = soup.select_one('#main-content h1 span')
    title = title.text.strip() if title else None

    date = soup.select_one('ds-item-page-date-field span')
    date = date.text.strip() if date else None

    authors = soup.select('ds-metadata-representation-list ds-metadata-field-wrapper div div a')
    author_list = [a.text.strip() for a in authors if a.text.strip()] if authors else []

    abstract = soup.select_one('ds-item-page-abstract-field span')
    abstract = abstract.text.strip() if abstract else None

    keywords = soup.select_one('ds-generic-item-page-field:nth-child(4) span')
    keywords = keywords.text.strip().replace("\n", ", ") if keywords else None

    pdf_tag = soup.select_one('ds-file-download-link > a')
    pdf_url = pdf_tag['href'] if pdf_tag else None
    full_pdf_url = API_BASE + pdf_url if pdf_url and pdf_url.startswith("/") else pdf_url
    content_url = full_pdf_url.replace("/download", "/content") if full_pdf_url else None

    if content_url:
        try:
            pdf_response = requests.get(content_url)
            pdf_response.raise_for_status()
            os.makedirs(save_dir, exist_ok=True)
            safe_title = re.sub(r'[\\/*?:"<>|]', "_", title or "unknown_title")
            filename = f"{safe_title}.pdf"
            filepath = os.path.join(save_dir, filename)
            with open(filepath, 'wb') as f:
                f.write(pdf_response.content)
        except Exception as e:
            print(f"[!] PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        print("[!] PDF ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return pd.DataFrame({
        'Title': [title],
        'Date': [date],
        'Authors': [', '.join(author_list)],
        'Abstract': [abstract],
        'Keywords': [keywords],
        'URL': [url],
        'PDF_URL': [full_pdf_url],
        'CONTENT_URL': [content_url]
    })

def main():
    print(f"ğŸ” HICSS {year} ë…¼ë¬¸ í¬ë¡¤ë§: í˜ì´ì§€ {start_page} ~ {end_page}")

    # ê¸°ì¡´ íŒŒì¼ì´ ìˆë‹¤ë©´ ë°ì´í„° ëˆ„ì 
    if os.path.exists(CSV_FILE):
        result_df = pd.read_csv(CSV_FILE)
    else:
        result_df = pd.DataFrame()

    for page_num in tqdm(range(start_page, end_page + 1), desc="ğŸ“„ í˜ì´ì§€ ì§„í–‰"):
        paper_links = extract_paper_links_from_page(page_num)

        if not paper_links:
            print(f"[!] í˜ì´ì§€ {page_num}ì— ë…¼ë¬¸ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        page_metadata = []
        for link in tqdm(paper_links, desc=f"ğŸ“¥ Page {page_num} ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ", leave=False):
            df = extract_paper_metadata(link)
            if df is not None and not df.empty:
                page_metadata.append(df)
            else:
                print(f"[!] ë§í¬ {link}ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨")

            time.sleep(1)  # ë…¼ë¬¸ ê°„ ëŒ€ê¸°

        if page_metadata:
            # í˜ì´ì§€ë³„ ë°ì´í„° ëˆ„ì  ë° ì €ì¥
            page_df = pd.concat(page_metadata, ignore_index=True)
            result_df = pd.concat([result_df, page_df], ignore_index=True)
            result_df.to_csv(CSV_FILE, index=False)
        else:
            print(f"[!] í˜ì´ì§€ {page_num}ì—ì„œ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        time.sleep(3)  # í˜ì´ì§€ ê°„ ëŒ€ê¸°

    print(f"\nâœ… ì§€ì •ëœ í˜ì´ì§€({start_page}-{end_page})ì˜ ë…¼ë¬¸ ì •ë³´ê°€ '{CSV_FILE}'ì— ëˆ„ì  ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main()


# 71~